{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install langgraph langchain langchain_community langchainhub langchain_groq langchain_huggingface bs4 tiktoken chromadb"
      ],
      "metadata": {
        "id": "70DoGE1yzj_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brief"
      ],
      "metadata": {
        "id": "92N3As34yle_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZjj4uLTvutc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "bdbb0adc-d1ad-4d7e-afdc-22edf4a0a198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from typing import Annotated, Literal, Sequence, TypedDict\n",
        "from langchain import hub\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import tools_condition\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = ''\n",
        "os.environ['HF_TOKEN'] = ''"
      ],
      "metadata": {
        "id": "TuKTGDx30g58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "TRXYWdDr5YIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(model_name='Gemma2-9b-It')"
      ],
      "metadata": {
        "id": "QXd-kX1X1ekd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering\"\n",
        "]"
      ],
      "metadata": {
        "id": "03Mco3_C6I-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [WebBaseLoader(url).load() for url in urls]"
      ],
      "metadata": {
        "id": "pnYfewIw6oIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "# Convert each dictionary to a Document instance\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs_list = [\n",
        "    Document(page_content=doc[\"page_content\"], metadata=doc.get(\"metadata\", {}))\n",
        "    if isinstance(doc, dict) else doc\n",
        "    for doc in docs_list\n",
        "]"
      ],
      "metadata": {
        "id": "gJ7Evc1J78Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=5)"
      ],
      "metadata": {
        "id": "fnAaF8WM7my7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_splits=text_splitter.split_documents(docs_list)\n"
      ],
      "metadata": {
        "id": "82IdLE958qL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(\n",
        "    documents = doc_splits,\n",
        "    collection_name = 'rag-chrome',\n",
        "    embedding = embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "_55nu2qM8-E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "yrjyVLD2-95p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"retrieve_blog_posts\",\n",
        "    \"Search and return the information about lilian wang blog posts.\"\n",
        ")"
      ],
      "metadata": {
        "id": "c3R-RyGdAVSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [retriever_tool]"
      ],
      "metadata": {
        "id": "UvYg_kA0A1ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages : Annotated[Sequence[BaseMessage], add_messages]"
      ],
      "metadata": {
        "id": "Jefb2qRJ1cx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AI_Assistant(state:AgentState):\n",
        "  print(\"----CALL Agent----\")\n",
        "  messages = state[\"messages\"]\n",
        "  llm_with_tool = llm.bind_tools(tools)\n",
        "  response = llm_with_tool.invoke(messages)\n",
        "  return {\"messages\":[response]}"
      ],
      "metadata": {
        "id": "j6BeYGZT2bsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rewrite(state:AgentState):\n",
        "  print(\"---Transform Query-----\")\n",
        "  messages = state['messages']\n",
        "  question = messages[0].content\n",
        "\n",
        "  msg = [\n",
        "      HumanMessage(\n",
        "          content = f\"\"\" \\n\n",
        "  look at the input and try to reason about the underlying semantic intent / meaning. \\n\n",
        "  Here is the initital question:\n",
        "  \\n ---------------- \\n\n",
        "  {question}\n",
        "  \\n ---------------- \\n\n",
        "  Formulate an improved question : \"\"\",\n",
        "      )\n",
        "  ]\n",
        "\n",
        "  response = llm.invoke(msg)\n",
        "  return {\"messages\":[response]}\n",
        "\n"
      ],
      "metadata": {
        "id": "Rx117wo24Bys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(state:AgentState):\n",
        "  print(\"----GENERATE-----\")\n",
        "  messages = state[\"messages\"]\n",
        "  question = messages[0].content\n",
        "  last_message = messages[-1]\n",
        "  docs = last_message.content\n",
        "\n",
        "  prompt = hub.pull('rlm/rag-prompt')\n",
        "\n",
        "  rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "  response = rag_chain.invoke({\n",
        "      \"context\": docs,\n",
        "      \"question\": question\n",
        "  })\n",
        "\n",
        "  return {\"messages\":[response]}"
      ],
      "metadata": {
        "id": "pPFvuV7b4FXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class grade (BaseModel):\n",
        "  binary_score: str = Field(description=\"Relevance Score 'yes' or 'no' \")"
      ],
      "metadata": {
        "id": "zWttaLMHETYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grade_documents(state:AgentState)->Literal[\"Output_Generator\", \"Query_Rewriter\"]:\n",
        "  llm_with_structure_op = llm.with_structured_output(grade)\n",
        "  prompt = PromptTemplate(\n",
        "      template = \"\"\" You are a grader assessing the relevance of a retreived document to a user question. \\n\n",
        "      Here is the retreived document: \\n \\n {context} \\n \\n\n",
        "      Here is the user question: {question} \\n\n",
        "      If the document contains keyword(s) or semantic meaning related to the user's question, mark it as relevant.\n",
        "      Give a 'yes' or 'no' answer to show if the document is relevant to the question.\"\"\",\n",
        "      input_variables = [\"context\",\"question\"]\n",
        "  )\n",
        "\n",
        "  chain = prompt | llm_with_structure_op\n",
        "  messages = state['messages']\n",
        "  print(f'message from the grader:{messages}')\n",
        "  last_message = messages[-1]\n",
        "  question = messages[0].content\n",
        "  docs = last_message.content\n",
        "  scored_result = chain.invoke({\n",
        "      \"context\": docs,\n",
        "      \"question\": question\n",
        "  })\n",
        "  score  = scored_result.binary_score\n",
        "\n",
        "  if score =='yes':\n",
        "    print('----Decision: DOCS Relevant ----')\n",
        "    return 'generator'\n",
        "  else :\n",
        "    print ('Decision: not relevant')\n",
        "    return \"rewriter\""
      ],
      "metadata": {
        "id": "XKYkCpTiE17D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"ai_assistant\",AI_Assistant)\n",
        "retreiver = ToolNode([retriever_tool])\n",
        "workflow.add_node(\"retrieve\",retriever)\n",
        "workflow.add_node(\"rewriter\",rewrite)\n",
        "workflow.add_node(\"generator\",generate)"
      ],
      "metadata": {
        "id": "7D3JSiB94KaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.add_edge(START,'ai_assistant')\n",
        "workflow.add_conditional_edges(\"ai_assistant\",tools_condition,\n",
        "                               {\"tools\":\"retrieve\",END:END})       # we are using a predefined method called tools_condition which checks the tool condition if YES then retreive otherwise END node\n",
        "\n",
        "workflow.add_conditional_edges('retrieve',grade_documents,\n",
        "                               {'rewriter':'rewriter',\"generator\":'generator'})\n",
        "\n",
        "workflow.add_edge(\"rewriter\",'ai_assistant')\n",
        "workflow.add_edge(\"generator\",END)\n"
      ],
      "metadata": {
        "id": "0QYGB13L4WEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app=workflow.compile()"
      ],
      "metadata": {
        "id": "qA_lAsZf45WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "try:\n",
        "  display(Image((app.get_graph().draw_mermaid_png())))\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "RVfwE_is51CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app.invoke({\"messages\": [HumanMessage(content=\"What is an Autonomous Agent?\")]})"
      ],
      "metadata": {
        "id": "2MUUkoLH473p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brief about the code flow .\n",
        "\n",
        "- First the input comes from the user and goes to the AI - Assistant and branch out to 2 nodes. If the LLM decides to call the tool (i.e retrieval tool here) then the flow will shift to the retreiver node otherwise it will shift to the end node and terminate\n",
        "\n",
        "- Now in the case of retriver node , it will fetch the relevant documents and then grade them. If relevant documents are found then the flow shift to generate node otherwise it will shift to the rewrite node.\n",
        "\n",
        "- now in the rewriter node the previous question, is recieved and then it is reframed in hope for a better output and then passed again to the starting node which is the AI_Assistant.\n"
      ],
      "metadata": {
        "id": "Lbkn9XoFHrak"
      }
    }
  ]
}
